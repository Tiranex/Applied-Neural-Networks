{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "shape = (6,7)\n",
    "\n",
    "class connect_x:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board_height = shape[0]\n",
    "        self.board_width = shape[1]\n",
    "        self.board_state = np.zeros([self.board_height, self.board_width], dtype=np.int8)\n",
    "        self.players = {'p1': 1, 'p2': 2}\n",
    "        self.isDone = False\n",
    "        self.reward = {'win': 1, 'draw': 0.5, 'lose': -1}\n",
    "    \n",
    "    def render(self):\n",
    "        rendered_board_state = self.board_state.copy().astype(str)\n",
    "        rendered_board_state[self.board_state == 0] = ' '\n",
    "        rendered_board_state[self.board_state == 1] = 'O'\n",
    "        rendered_board_state[self.board_state == 2] = 'X'\n",
    "        display(pd.DataFrame(rendered_board_state))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "        \n",
    "    def get_available_actions(self):\n",
    "        available_cols = []\n",
    "        for j in range(self.board_width):\n",
    "            if np.sum([self.board_state[:, j] == 0]) != 0:\n",
    "                available_cols.append(j)\n",
    "        return available_cols\n",
    "    \n",
    "    def check_game_done(self, player):\n",
    "        if player == 'p1':\n",
    "            check = '1 1 1 1'\n",
    "        else:\n",
    "            check = '2 2 2 2'\n",
    "        \n",
    "        # check vertically then horizontally\n",
    "        for j in range(self.board_width):\n",
    "            if check in str(self.board_state[:, j]):\n",
    "                self.isDone = True\n",
    "        for i in range(self.board_height):\n",
    "            if check in str(self.board_state[i, :]):\n",
    "                self.isDone = True\n",
    "        \n",
    "        # check left diagonal and right diagonal\n",
    "        for k in range(0, self.board_height - 4 + 1):\n",
    "            left_diagonal = np.array([self.board_state[k + d, d] for d in \\\n",
    "                            range(min(self.board_height - k, min(self.board_height, self.board_width)))])\n",
    "            right_diagonal = np.array([self.board_state[d + k, self.board_width - d - 1] for d in \\\n",
    "                            range(min(self.board_height - k, min(self.board_height, self.board_width)))])\n",
    "            if check in str(left_diagonal) or check in str(right_diagonal):\n",
    "                self.isDone = True\n",
    "        for k in range(1, self.board_width - 4 + 1):\n",
    "            left_diagonal = np.array([self.board_state[d, d + k] for d in \\\n",
    "                            range(min(self.board_width - k, min(self.board_height, self.board_width)))])\n",
    "            right_diagonal = np.array([self.board_state[d, self.board_width - 1 - k - d] for d in \\\n",
    "                            range(min(self.board_width - k, min(self.board_height, self.board_width)))])\n",
    "            if check in str(left_diagonal) or check in str(right_diagonal):\n",
    "                self.isDone = True\n",
    "        \n",
    "        if self.isDone:\n",
    "            return self.reward['win']\n",
    "        # check for draw\n",
    "        elif np.sum([self.board_state == 0]) == 0:\n",
    "            self.isDone = True\n",
    "            return self.reward['draw']\n",
    "        else:\n",
    "            return 0.\n",
    "        \n",
    "    def make_move(self, a, player):\n",
    "        # check if move is valid\n",
    "        if a in self.get_available_actions():\n",
    "            i = np.sum([self.board_state[:, a] == 0]) - 1\n",
    "            self.board_state[i, a] = self.players[player]\n",
    "        else:\n",
    "            print('Move is invalid')\n",
    "            self.render()\n",
    "\n",
    "        reward = self.check_game_done(player)\n",
    "        \n",
    "        # give feedback as new state and reward\n",
    "        return self.board_state.copy().reshape((1, shape[0], shape[1], 1)), reward\n",
    "\n",
    "env = connect_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# memory block for deep q learning\n",
    "class replayMemory:\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "        \n",
    "    def dump(self, transition_tuple):\n",
    "        self.memory.append(transition_tuple)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = replayMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "class Player(Model):\n",
    "  def __init__(self):\n",
    "    super(Player, self).__init__()\n",
    "\n",
    "\n",
    "    linear_input_size =6 *7 * 32\n",
    "    input_shape = (6,7,1)\n",
    "    self.model = tf.keras.Sequential([\n",
    "        layers.Conv2D(32, 5, padding='same', input_shape=input_shape, activation='relu'),\n",
    "        layers.Conv2D(32, 5, padding='same', activation='relu'),\n",
    "        layers.Conv2D(32, 5, padding='same', activation='relu'),\n",
    "        layers.Conv2D(32, 5, padding='same', activation='relu'),\n",
    "        layers.Conv2D(32, 5, padding='same', activation='relu'),\n",
    "        layers.Conv2D(32, 5, padding='same', activation='relu'),\n",
    "        layers.Conv2D(32, 5, padding='same', activation='relu'),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(linear_input_size, activation='relu'),\n",
    "        layers.Dense(linear_input_size, activation='relu'),\n",
    "        layers.Dense(linear_input_size, activation='relu'),\n",
    "        layers.Dense(shape[1], activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "policy_net = Player()\n",
    "policy_net.compile(optimizer='adam', loss='mean_squared_error')\n",
    "target_net = Player()\n",
    "target_net.compile(optimizer='adam', loss='mean_squared_error')\n",
    "target_net.set_weights(policy_net.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 7])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Player().call(env.make_move(0, 'p1')[0].reshape(1, 6,7)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Parameters\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 2000\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.999\n",
    "\n",
    "def select_action(state, available_actions, steps_done=None, training=True):\n",
    "    # batch and color channel\n",
    "    epsilon = random.random()\n",
    "    if training:\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1 * steps_done / EPS_DECAY)\n",
    "    else:\n",
    "        eps_threshold = 0\n",
    "    \n",
    "    # follow epsilon-greedy policy\n",
    "    if epsilon > eps_threshold:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # action recommendations from policy net\n",
    "            r_actions = policy_net(state)[0, :]\n",
    "            state_action_values = [r_actions[action] for action in available_actions]\n",
    "            argmax_action = np.argmax(state_action_values)\n",
    "            greedy_action = available_actions[argmax_action]\n",
    "            return greedy_action\n",
    "    else:\n",
    "        return random.choice(available_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    state_batch, action_batch, reward_batch, next_state_batch = zip(*[(np.expand_dims(m[0], axis=0), \\\n",
    "                                        [m[1]], m[2], np.expand_dims(m[3], axis=0)) for m in transitions])\n",
    "\n",
    "    \n",
    "    # for assigning terminal state value = 0 later\n",
    "    non_final_mask = tuple(map(lambda s_: s_[0] is not None, next_state_batch))\n",
    "    non_final_next_state = tf.concat([s_ for s_ in next_state_batch if s_[0] is not None])\n",
    "    \n",
    "    # prediction from policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # truth from target_net, initialize with zeros since terminal state value = 0\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    # tensor.detach() creates a tensor that shares storage with tensor that does not require grad\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_state).max(1)[0].detach()\n",
    "    # compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1)) # torch.tensor.unsqueeze returns a copy\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random agent\n",
    "def random_agent(actions):\n",
    "    return random.choice(actions)\n",
    "\n",
    "# win rate test\n",
    "def win_rate_test():\n",
    "    win_moves_taken_list = []\n",
    "    win = []\n",
    "    for i in range(100):\n",
    "        env.reset()\n",
    "        win_moves_taken = 0\n",
    "\n",
    "        while not env.isDone:\n",
    "            state = env.board_state.copy().reshape((1, shape[0], shape[1], 1))\n",
    "            available_actions = env.get_available_actions()\n",
    "            action = select_action(state, available_actions, training=False)\n",
    "            state, reward = env.make_move(action, 'p1')\n",
    "            win_moves_taken += 1\n",
    "\n",
    "            if reward == 1:\n",
    "                win_moves_taken_list.append(win_moves_taken)\n",
    "                win.append(1)\n",
    "                break\n",
    "\n",
    "            available_actions = env.get_available_actions()\n",
    "            action = random_agent(available_actions)\n",
    "            state, reward = env.make_move(action, 'p2')\n",
    "\n",
    "    return sum(win)/100, sum(win_moves_taken_list)/len(win_moves_taken_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.77, 6.753246753246753)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_rate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid resetting\n",
    "steps_done = 0\n",
    "training_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Missing required positional argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m     state_p1 \u001b[38;5;241m=\u001b[39m state_p2_\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# update the target network, copying all weights and biases in DQN\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m TARGET_UPDATE \u001b[38;5;241m==\u001b[39m TARGET_UPDATE \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[79], line 11\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# for assigning terminal state value = 0 later\u001b[39;00m\n\u001b[0;32m     10\u001b[0m non_final_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m s_: s_[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, next_state_batch))\n\u001b[1;32m---> 11\u001b[0m non_final_next_state \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# prediction from policy_net\u001b[39;00m\n\u001b[0;32m     14\u001b[0m state_action_values \u001b[38;5;241m=\u001b[39m policy_net(state_batch)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_batch)\n",
      "File \u001b[1;32mf:\\VS-Code\\Applied-Neural-Networks\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mf:\\VS-Code\\Applied-Neural-Networks\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1254\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iterable_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m replace_iterable_params(args, kwargs, iterable_params)\n\u001b[1;32m-> 1254\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mapi_dispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1256\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mTypeError\u001b[0m: Missing required positional argument"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "num_episodes = 20000\n",
    "# control how lagged is target network by updating every n episodes\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "for i in range(num_episodes): \n",
    "    env.reset()\n",
    "    state_p1 = env.board_state.copy().reshape((1, shape[0], shape[1], 1))\n",
    "\n",
    "    # record every 20 epochs\n",
    "    if i % 20 == 19:\n",
    "        win_rate, moves_taken = win_rate_test()\n",
    "        training_history.append([i + 1, win_rate, moves_taken])\n",
    "        th = np.array(training_history)\n",
    "        # print training message every 200 epochs\n",
    "        if i % 200 == 199:\n",
    "            print('Episode {}: | win_rate: {} | moves_taken: {}'.format(i, th[-1, 1], th[-1, 2]))\n",
    "\n",
    "    for t in count():\n",
    "        available_actions = env.get_available_actions()\n",
    "        action_p1 = select_action(state_p1, available_actions, steps_done)\n",
    "        steps_done += 1\n",
    "        state_p1_, reward_p1 = env.make_move(action_p1, 'p1')\n",
    "        \n",
    "        if env.isDone:\n",
    "            if reward_p1 == 1:\n",
    "                # reward p1 for p1's win\n",
    "                memory.dump([state_p1, action_p1, 1, None])\n",
    "            else:\n",
    "                # state action value tuple for a draw\n",
    "                memory.dump([state_p1, action_p1, 0.5, None])\n",
    "            break\n",
    "        \n",
    "        available_actions = env.get_available_actions()\n",
    "        action_p2 = random_agent(available_actions)\n",
    "        state_p2_, reward_p2 = env.make_move(action_p2, 'p2')\n",
    "        \n",
    "        if env.isDone:\n",
    "            if reward_p2 == 1:\n",
    "                # punish p1 for (random agent) p2's win \n",
    "                memory.dump([state_p1, action_p1, -1, None])\n",
    "            else:\n",
    "                # state action value tuple for a draw\n",
    "                memory.dump([state_p1, action_p1, 0.5, None])\n",
    "            break\n",
    "        \n",
    "        # punish for taking too long to win\n",
    "        memory.dump([state_p1, action_p1, -0.05, state_p2_])\n",
    "        state_p1 = state_p2_\n",
    "        \n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "        \n",
    "    # update the target network, copying all weights and biases in DQN\n",
    "    if i % TARGET_UPDATE == TARGET_UPDATE - 1:\n",
    "        target_net.set_weights(policy_net.get_weights())\n",
    "\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
